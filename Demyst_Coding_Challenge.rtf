{\rtf1\ansi\ansicpg1252\cocoartf2709
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica-Bold;\f1\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww17220\viewh15260\viewkind0
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b\fs24 \cf0 ## Problem1 - Parse fixed width file
\f1\b0 \
\
from pyspark.sql import SparkSession\
from pyspark.sql.types import StructType, StructField, IntegerType, StringType\
\
# Initialize Spark session\
spark = SparkSession.builder \\\
    .appName("Fixed width to CSV") \\\
    .getOrCreate()\
\
# Define the schema for the DataFrame\
schema = StructType([\
    StructField("id", IntegerType(), True),\
    StructField("name", StringType(), True),\
    StructField("age", IntegerType(), True)\
])\
\
# Define the fixed-width field lengths\
field_widths = [5, 20, 3]\
\
# Read the fixed-width file into an RDD\
rdd = spark.sparkContext.textFile(\'93file_path/input_file.txt")\
\
# Function to parse each line\
def parse_line(line):\
    id = int(line[0:5].strip())\
    name = line[5:25].strip()\
    age = int(line[25:28].strip())\
    return (id, name, age)\
\
# Parse the RDD\
parsed_rdd = rdd.map(parse_line)\
\
# Convert the RDD to a DataFrame\
df = spark.createDataFrame(parsed_rdd, schema)\
\
# Write the DataFrame to a CSV file\
df.write.csv(\'93file_path/output_file.csv", header=True)\
\
# Stop the Spark session\
spark.stop()\
\
**********************************************************************************************\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0

\f0\b \cf0 ## Problem2 - Anonymise the data
\f1\b0 \
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 \
from pyspark.sql import SparkSession\
from faker import Faker\
from pyspark.sql.functions import udf\
from pyspark.sql.types import StringType\
\
# Initialize Spark session\
spark = SparkSession.builder \\\
    .appName("Anonymize Data") \\\
    .getOrCreate()\
\
# Read the CSV file\
df = spark.read.csv(\'93file_path/customer.csv", header=True, inferSchema=True)\
\
# Initialize Faker\
fake = Faker()\
\
# Define UDFs for generating fake data\
def fake_first_name():\
    return fake.firstname()\
\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 def fake_last_name():\
    return fake.lastname()\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 \
def fake_address():\
    return fake.address()\
\
fake_first_name_udf = udf(fake_first_name, StringType())\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 fake_last_name_udf = udf(fake_last_name, StringType())\
\pard\tx566\tx1133\tx1700\tx2267\tx2834\tx3401\tx3968\tx4535\tx5102\tx5669\tx6236\tx6803\pardirnatural\partightenfactor0
\cf0 fake_address_udf = udf(fake_address, StringType())\
\
# Apply the UDFs to the DataFrame\
anonymized_df = df.withColumn(\'93firstname", fake_first_name_udf()) \\\
		 .withColumn(\'93lastname", fake_last_name_udf()) \\\
                  .withColumn("address\'94, fake_address_udf())\
\
# Write the anonymized data back to a CSV file\
anonymized_df.write.csv("path/to/your/anonymized_customer.csv", header=True)\
\
# Stop the Spark session\
spark.stop()\
}